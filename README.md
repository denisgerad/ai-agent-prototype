# AI Agent Prototype with Memory and Tool Use

A lightweight **AI agent proof-of-concept** designed to demonstrate
**conversation memory, agentic reasoning, and dynamic tool usage**
using a locally hosted LLM.

Rather than focusing on model performance or UI polish, this project
explores how an AI agent **decides when and how to use different tools**
â€” including PDF-based retrieval (RAG), web scraping, web search, and
external APIs â€” while maintaining conversational context.

The application is available in both **CLI** and **Streamlit web interface**
forms.

---

## ğŸŒŸ Agent Capabilities

- **PDF Question Answering (RAG)**  
  Uses vector embeddings to retrieve relevant information from PDF documents
  as one of several tools available to the agent.

- **Conversation Memory**  
  Maintains conversational state across multiple turns, enabling follow-up
  questions and contextual reasoning.

- **Web Scraping**  
  Extracts and reasons over text content from arbitrary webpages.

- **Web Search**  
  Performs live internet searches using DuckDuckGo (Streamlit version).

- **Weather Information**  
  Retrieves real-time weather data for user-specified locations.

- **Agent-Based Architecture**  
  The agent dynamically selects tools based on user intent using a
  ReAct-style reasoning loop.

---

## ğŸ› ï¸ Tech Stack

- **LangChain** â€“ Framework for building agentic LLM applications  
- **Ollama** â€“ Local LLM inference (Mistral model)  
- **ChromaDB** â€“ Vector database for document embeddings  
- **HuggingFace Embeddings** â€“ Sentence-transformer models for text embeddings  
- **Streamlit** â€“ Optional web-based chat interface  
- **PyPDF** â€“ PDF document processing  

---

## ğŸ“‹ Prerequisites

- Python 3.8+
- Ollama installed and running
- Mistral model installed in Ollama:
  ```bash
  ollama pull mistral


#working code for RetrievalQA
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM
from langchain_classic.chains import RetrievalQA

# -------- CONFIG --------
PDF_PATH = "sample.pdf"
OLLAMA_MODEL = "mistral"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHROMA_DIR = "./chroma_db"
# ------------------------

def main():
    print("üìÑ Loading PDF...")
    loader = PyPDFLoader(PDF_PATH)
    documents = loader.load()

    print("‚úÇÔ∏è Chunking documents...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(documents)

    print("üî¢ Creating embeddings...")
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL
    )

    print("üì¶ Creating vector store...")
    vectordb = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_DIR
    )

    print("ü§ñ Initializing LLM...")
    llm = OllamaLLM(model=OLLAMA_MODEL)

    print("üîç Creating RetrievalQA chain...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    print("\n‚úÖ Ready! Ask questions (type 'exit' to quit)\n")

    while True:
        query = input("‚ùì Question: ")
        if query.lower() == "exit":
            break

        result = qa(query)
        print("\nüí° Answer:\n", result["result"], "\n")

if __name__ == "__main__":
    main()
